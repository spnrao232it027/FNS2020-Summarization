{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'transformers' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "! git clone https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd transformers && git reset --hard 143738214"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.13\n"
     ]
    }
   ],
   "source": [
    "! python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "WARNING:tensorflow:From c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "\n",
    "# Initialize our model\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"orzhan/t5-long-extract\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"orzhan/t5-long-extract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filenameWithPath:str)->str:\n",
    "  '''\n",
    "  This function reads the content of a file.\n",
    "\n",
    "  Parameters:\n",
    "  filenameWithPath : str\n",
    "    Absolute Path of the file which is to be read.\n",
    "\n",
    "  Returns:\n",
    "  text : str\n",
    "    The content of the read file. \n",
    "\n",
    "  '''\n",
    "  file = open(filenameWithPath,\"r\")\n",
    "  text = file.read().strip()\n",
    "  text = re.sub(\"\\n\",\" \",text)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "  '''\n",
    "  The preprocessing function which parses raw text and manages to clean the raw text. This module removes websites/URLs, Email IDs, redundant spaces, extra periods, dashes and commas. It also replace some special unicode apostrope, qoutation symbols with ' and  \". This module also ads extra space after period, question mark and exclamation mark.\n",
    "\n",
    "  Parameters:\n",
    "    text : str\n",
    "      The raw text to preprocess.\n",
    "\n",
    "  Returns:\n",
    "    text : str\n",
    "      The preprocessed text. \n",
    "  '''\n",
    "  # text = text.lower()\n",
    "  text = re.sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", \" \", text)\n",
    "  text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', \" \",text)\n",
    "  text = re.sub(\"\\u2019\", '\\'', text)\n",
    "  text = re.sub(\"\\u2018\", '\\'', text)\n",
    "  text = re.sub(\"\\u201C\", '\\\"', text)\n",
    "  text = re.sub(\"\\u201D\", '\\\"', text)\n",
    "  # text = re.sub(\"[$]\", \"dollar \", text)\n",
    "  # text = re.sub(\"[£]\", \"pound \", text)\n",
    "  # text = re.sub(\"[%]\", \" percent\", text)\n",
    "  # text = re.sub(r\"[^a-zA-Z0-9?!.,’-]\", ' ', text)\n",
    "  text = re.sub(r\"([?!])\", r\" \\1 \", text)\n",
    "  text = re.sub(r',+', ',', text)\n",
    "  text = re.sub(r'[-]+', ' ', text)\n",
    "  text = re.sub(r'([a-z])(?=[.,])', r'\\1 ', text)\n",
    "  text = re.sub(r'\\.{2,}', '.', text)\n",
    "  text = re.sub(r\"\\s+\", \" \", text)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSentences(text):\n",
    "    '''\n",
    "    This function uses the nltk library to sentence tokenize a given corpus or document.\n",
    "\n",
    "    Parameters:\n",
    "    text : \n",
    "        The corpus or document string to split into sentences.\n",
    "\n",
    "    Returns: \n",
    "        A sentence tokenized copy of *text* using NLTK's default tokenizer. A list of sentences.\n",
    "\n",
    "    '''\n",
    "    return nltk.tokenize.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createChunks(sentences):\n",
    "  '''\n",
    "  This module splits the entire corpus into chunks. A chunk is a list of sentences from the corpus. This module add a sentence to a chunk if the tokenized sentence doesn't exceed the models' single sentence length. Else it adds the senence to next chunk.\n",
    "\n",
    "  Parameters:\n",
    "  sentences : list\n",
    "    A list of containing sentences.\n",
    "\n",
    "  Returns:\n",
    "  chunks : list\n",
    "    A list of containing the chunks of the document.\n",
    "  '''\n",
    "  length = 0\n",
    "  chunk = \"\"\n",
    "  chunks = []\n",
    "  count = -1\n",
    "  for sentence in sentences:\n",
    "    # print(\"Sent \", sentence)\n",
    "    count += 1\n",
    "    combined_length = len(tokenizer.tokenize(sentence)) + length # add the no. of sentence tokens to the length counter\n",
    "\n",
    "    if combined_length  < tokenizer.max_len_single_sentence - 3 : # if it doesn't exceed\n",
    "      chunk += sentence + \" \" # add the sentence to the chunk\n",
    "      length = combined_length # update the length counter\n",
    "      # print(\"length\",length)\n",
    "      # if it is the last sentence\n",
    "      if count == len(sentences) - 1:\n",
    "        chunks.append(chunk.strip()) # save the chunk\n",
    "      # print(\"if\", chunks)\n",
    "    else:\n",
    "      chunks.append(chunk.strip()) # save the chunk\n",
    "      # print(\"else\", chunks)\n",
    "\n",
    "      # reset\n",
    "      length = 0\n",
    "      chunk = \"\"\n",
    "\n",
    "      # take care of the overflow sentence\n",
    "      chunk += sentence + \" \"\n",
    "      length = len(tokenizer.tokenize(sentence))\n",
    "  return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chunk_summary(chunk):\n",
    "    '''\n",
    "    This module generates the summary for a given document. Prior to passing the document to model, it appends 'summarize: ' prefix to document to let the model know that the task is summarization. It restricts the generated summary to 100 tokens.\n",
    "\n",
    "    Parameters:\n",
    "    chunk : str\n",
    "        A chunk is group of sentences such that the tokenized lenght of chunk doesn't exceed the token limit of model.\n",
    "\n",
    "    Returns:\n",
    "    summary : str\n",
    "        It is the summary which model generates for the given text chunk. \n",
    "    '''\n",
    "    input_ids = tokenizer.encode(\"summarize: \" + chunk, return_tensors=\"pt\",max_length=tokenizer.model_max_length, truncation=True)\n",
    "    summary_ids = model.generate(input_ids,max_length=100)\n",
    "\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(fileChunks):\n",
    "    '''\n",
    "    This function returns the complete summary for a chunked document or corpus of text.\n",
    "\n",
    "    Parameters:\n",
    "    fileChunks: list\n",
    "        A list of chunk where chunk is group of sentences in a corpus such that tokenized length of chunk doesn't exceed the model limit.\n",
    "\n",
    "    Returns:\n",
    "        Concatenated Summary as *str* for every chunk. \n",
    "    '''\n",
    "    summaries = []\n",
    "\n",
    "    for chunk in fileChunks:\n",
    "        summ = generate_chunk_summary(chunk)\n",
    "        summaries.append(summ)\n",
    "\n",
    "    return \" \".join(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap_summary(generated_summary:str)->str:\n",
    "    '''\n",
    "    This function caps the generated summary to 1000 words. \n",
    "\n",
    "    Parameters:\n",
    "    generated_summary : str\n",
    "        System generated summary.\n",
    "\n",
    "    Returns:\n",
    "        Returns Summary as str after capping.\n",
    "    '''\n",
    "    sentences = nltk.sent_tokenize(generated_summary)\n",
    "    new_summary=[]\n",
    "    target_word=1250\n",
    "    words_added=0\n",
    "    for sentence in sentences:\n",
    "        words= nltk.word_tokenize(sentence)\n",
    "        if words_added + len(words) <= target_word:\n",
    "            words_added+=len(words)\n",
    "            new_summary.append(sentence)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return \"\".join(new_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize():\n",
    "    '''\n",
    "    This function generates the summary for all the documents in *data-dir* . It generates the summary or the document and writes the summary in *generated_summary* folder. The convention for written file is *filename_sysSumm.txt*\n",
    "    '''\n",
    "    data_dir = os.getenv('VAL_AR')\n",
    "    for file in os.listdir(data_dir):\n",
    "        \n",
    "        fileContent = read_file(os.path.join(data_dir,file))\n",
    "        filePreprocessed = preprocessing(fileContent)\n",
    "        fileSentences = createSentences(filePreprocessed)\n",
    "        fileChunks = createChunks(fileSentences)\n",
    "        summary = generate_summary(fileChunks)\n",
    "        summary = cap_summary(summary)\n",
    "        filename = file[:-4]\n",
    "\n",
    "        with open(f\"./{os.getenv('TARGET_DIR')}/{filename}_sysSumm.txt\",\"w+\") as f:\n",
    "            f.write(summary)\n",
    "\n",
    "        # break\n",
    "        \n",
    "\n",
    "\n",
    "summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'25695 19 March 2018 3:29 PM Proof 7 25695 19 March 2018 3:29 PM Proof 7 annual report 2017 Bodycote plc annual report 2017 | Stock code: BOY 25695 19 March 2018 3:29 PM Proof 7 At a glance Our structure The Group operates 187 facilities around the world which are organised into customer focused division Throughout this report you will see illustrations which link our business and strategy: Strategy & Core Values Key Performance Indicators £ Aerospace, Defence & Energy Rapid growth countries Customer service Automotive & General Industrial Technology Core values Return on capital employed Headline earnings per share Accident frequency Return on sales Headline operating cash flow Carbon footprint Headline operating profit and headline profit before taxation exclude amortisation of acquired intangibles of £4.5m (2016: £4.5m) and acquisition costs of £nil (2016: £0.6m).2.Return on sales is defined as headline operating profit as a percentage of revenue.This Strategic report has been prepared for the Group as a whole and therefore gives greater emphasis to those matters which are significant to Bodycote plc and its subsidiary undertakings when viewed as a whole.The Strategic report discusses the following areas:   Global network   Chairman\\'s statement Global network Group revenue by market sector – £m Aerospace and Defence 160.7 56.2 Automotive 205.2 General Industrial 268.1 Energy Total 690.2 Group revenue by geography – £m Western Europe 384.9 251.2 Emerging markets 54.1 Total 690.2 North America Revenue by market sector Bodycote is experienced in all major market sectors I have spent time with all of my Board colleagues and am confident that I have joined a well balanced Board that, in terms of governance, is functioning effectively. \"A. C. Quinn CBE Chairman I am pleased to be contributing to the annual report for the first time as Bodycote\\'s Chairman.06 Bodycote plc annual report for the year ended 31 December 2017 25695 19 March 2018 3:29 PM Proof 7 The Board has been further strengthened by the appointment of Lili Chahbazi, who joined as a Non Executive Director on 1 January 2018.Lili is an experienced strategy consultant and, since 2008, a global partner in the results of these investments, together with the continued diligence in operational efficiency and a more benign macro environment all combined to deliver another good set of results for the Group in 2017.Bodycote remains well placed to capitalise on the value generating investments available to it and the prospects for the Group are excellent Growth in onshore oil & gas saw a strong sequential increase in 2017, driven predominantly by demand from unconventional drilling activity in the Permian Basin.In the rest of the energy sector, subsea revenues continued to decline and large frame industrial gas turbines (IGT) are also in retreat following cutbacks at Also impacting our HIP Services business was an unplanned outage during the second half (which was fully resolved by year end).Forecast growth in all the Specialist Technologies looks strong.The five sites we acquired in 2016 are performing well.They are all Classical Heat Treatment sites within the AGI division and contributed £23.0m of revenue in 2017, The Group has a minimum 20% hurdle rate return when looking at investments.During 2017 we made further progress against our strategy, delivering higher Group return on sales of 18.0%.We believe there is still the opportunity to further improve from both management led initiatives, particularly in our AGI division, and growth in our higher return Specialist Technologies Strong growth was achieved through contributions from contract wins on automotive and aerospace programmes, excellent growth in Emerging Markets (where our investments are yielding good returns), and broad based growth across the general industrial sectors.an element of which was due to some customer restocking.The Group\\'s revenue growth, combined with continued discipline on costs, helped lift Bodycote seeks to secure service specific arrangements with our customers which provide protection from supply disruption by leveraging Bodycote\\'s unique facility network.Bodycote\\'s global network of 187 market focused facilities (see pages 4 and 5) in 23 countries brings economies of scale, particularly for logistics and equipment utilisation Creating value For customers For Bodycote For investors   Value adding services.Global supplier which can meet multiple processing needs.Access to entire Bodycote knowledge base and expertise.Cost and environmental benefits versus in house operations.Mutually beneficial customer relationships Divisional customer focus Organising our technologies and resources to align with the global and local requirements of the market sectors in which our customers operate and providing the highest levels of customer service in terms of quality, delivery, reliability and technical problem solving.T echnology Providing thermal processing services that are a vital link in the manufacturing supply chain, and value the surface is hardened via a specialised nitriding process during which nitrogen is diffused into the surface to increase resistance to in service stress and fatigue.After heat treatment, the masking is removed and the surface cleaned before inspection to ensure the part meets strict aerospace quality specifications.During use, a vehicle places heavy demand on its transmission, requiring a fast and reliable response to the drive controls.The gears require high strength and wear resistance in order to withstand the stresses applied to each gear during use.Bodycote\\'s heat treatment processes, in particular Low Pressure Carburising (LPC), enable modern transmissions to through the simultaneous application of heat and pressure, the HIP process eliminates internal porosity, improving fatigue strength, tensile ductility and fracture toughness.Surface hardness What is it?The ability of a material to resist deformation, scratching and indentation under force.some metals and alloys must operate at temperatures close to their melting point.Heat treatment enables them to perform at higher temperatures with little or no movement.CiD Corr I Dur ® (CiD) A proprietary thermochemical treatment for the simultaneous improvement of corrosion and wear resistance through the generation of a nitride oxide combination layer Performance Headline earnings per share increased by 12.2 pence (33.0%) during the year, from 37.0 pence to 49.2 pence.Definition Headline earnings per share is defined in note 10 to the financial statements.Performance Return on sales increased by 1.4 percentage points during the year, from 16.6% CO 2 equivalent emissions are calculated by taking electricity and gas usage in kilowatt hours and multiplying by country specific conversion factors provided by the International Energy Agency (IEA).Normalised emissions statistics restate prior year figures using current year country specific conversion (IEA) factors and current year average exchange rates.Revenue in 2017 was £273.1m, an increase of 4.7% (8.8% at actual rates), including a contribution of 0.8 percentage points to the growth from new facility investments.Civil aviation growth was underpinned by a strong UK performance.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run on single file\n",
    "\n",
    "\n",
    "valARDir= os.getenv('VAL_AR') # Init Validation annual report directory\n",
    "valGSDir = os.getenv('VAL_GS') # init validation gold summary diredtory\n",
    "\n",
    "testFile = os.path.join(valARDir,os.listdir(valARDir)[0]) # specifying which file to read, here 1st file\n",
    "testFileName = os.path.basename(testFile)[:-4] # get just the filename\n",
    "\n",
    "# get the gold summary for the previous file\n",
    "# Of all the gold summary corressponding to  above file, we pick the first one.\n",
    "gs = [file for file in os.listdir(valGSDir) if file.__contains__(testFileName)][0]\n",
    "gs = os.path.join(valGSDir,gs)\n",
    "\n",
    "fileContent = read_file(testFile) # read the annual report\n",
    "gs = read_file(gs) # read the gold summary\n",
    "filePreprocessed = preprocessing(fileContent)\n",
    "fileSentences = createSentences(filePreprocessed)\n",
    "fileChunks = createChunks(fileSentences)\n",
    "summary = generate_summary(fileChunks)\n",
    "summary=cap_summary(summary)\n",
    "summary # generated summary for the read annual report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': Score(precision=0.6546572934973638, recall=0.47634271099744246, fmeasure=0.5514433752775721), 'rouge2': Score(precision=0.31838170624450307, recall=0.23160588611644273, fmeasure=0.2681481481481481), 'rougeL': Score(precision=0.26362038664323373, recall=0.1918158567774936, fmeasure=0.2220577350111029)}\n",
      "Rouge 1 : \n",
      "\tPrecision : 0.6546572934973638\n",
      "\tRecall : 0.47634271099744246\n",
      "\tF1 Score : 0.5514433752775721\n",
      "Rouge 2 : \n",
      "\tPrecision : 0.31838170624450307\n",
      "\tRecall : 0.23160588611644273\n",
      "\tF1 Score : 0.2681481481481481\n",
      "Rouge L : \n",
      "\tPrecision : 0.26362038664323373\n",
      "\tRecall : 0.1918158567774936\n",
      "\tF1 Score : 0.2220577350111029\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1','rouge2' ,'rougeL'], use_stemmer=True)\n",
    "scores = scorer.score(gs,summary)\n",
    "# print(scores)\n",
    "\n",
    "# Print rouge metrics\n",
    "print(\"Rouge 1 : \")\n",
    "print(f\"\\tPrecision : {scores['rouge1'][0]}\")\n",
    "print(f\"\\tRecall : {scores['rouge1'][1]}\")\n",
    "print(f\"\\tF1 Score : {scores['rouge1'][2]}\")\n",
    "\n",
    "print(\"Rouge 2 : \")\n",
    "print(f\"\\tPrecision : {scores['rouge2'][0]}\")\n",
    "print(f\"\\tRecall : {scores['rouge2'][1]}\")\n",
    "print(f\"\\tF1 Score : {scores['rouge2'][2]}\")\n",
    "\n",
    "print(\"Rouge L : \")\n",
    "print(f\"\\tPrecision : {scores['rougeL'][0]}\")\n",
    "print(f\"\\tRecall : {scores['rougeL'][1]}\")\n",
    "print(f\"\\tF1 Score : {scores['rougeL'][2]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
